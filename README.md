Introduction

Researches have shown that performance of machine learning models is a function of the influence of imputed explanatory variables (Babel and Shinde, 2011; Farah, 2019; Firat, et al. 2009; Toth, et al. 2018). But datasets used in machine learning modeling come with irrelevant or less relevant and redundant variables, which can greatly reduce model performance. In order to aid machine learning performance, it might be necessary to delete unnecessary features, create new features, select and extract features that best correlate with the output variable. Feature extraction and selection functions are common techniques used to choose features which best aid machine learning modeling. These functions have been abundantly applied to socioeconomic data, weather data, demographic data, and property data, but sparsely applied to geospatial data (Ioannis, 2019; Honest, 2020; Yee et al., 2018; Doğan, and Uysal, 2018).
 
Geospatial data are information that are tied to a particular location in the earth. Features that are described by geospatial data are said to be georeferenced; that is, their position is coordinated by a reference system that helps in locating them with reference to the earth. Geospatial data includes information about the location, shape, and size of objects, as well as their relationships with other objects in space. Geospatial variables are important in machine learning modeling because they provide additional context and information that can improve the accuracy and effectiveness of models (Tingzon, et al., 2019; Xu et al., 2020).

Adding geospatial variables to datasets fed into machine learning models could give insights into the influence of geospatial component on phenomena (Patel, 2021). For example, data about water consumption by a class of people would usually include socioeconomic, demographic, weather and historical components. Since the people and their water sources are at definite locations on the earth surface, it means that their activities (water consumption in this case), and data that describe those activities, have geospatial component. The questions: Where? How far apart? And how high? can only and adequately be answered if the location is spatially referenced; that is, the location of the phenomena (the people and their water sources) have values from which their distance apart and height difference can be deduced. The geospatial component could add to knowledge if explored. For instance, how does distance from the nearest water source influence the per capita water consumption in a household? What is the impact of height difference between the household location and the nearest water source on water consumption? The above example shows the importance of geospatial factors in determining phenomena such as volume of water consumed in a household.

There have been researches that illustrate the importance of feature selection in machine learning modelling. Feature selection methods such as Pearson Correlation Coefficient, Information Gain, Relief-F attribute, Symmetrical Uncertainty, Recursive Feature Elimination and Principal Component Analysis are used and compared to choose an optimal subset of features to improve the oil spill detection systems (Mera et al., 2017) and accurately predict water demand (Oyebode, 2019). Yee et al., (2018) proposed feature selection to optimize geospatial features to predict sales. Jiménez et al., (2017) proposed a method of consolidating feature selection with regression model. In addition, past researches used different types of classifier to validate the performance of feature selection (Oreski, 2017; El-Kenawy et al., 2020; Lee and Shen, 2011). Yee et al., (2018) specifically provides evidence to prove that geospatial features play an indispensable role in sales analytics.

Adding more relevant features to datasets is beneficial to machine learning modeling, but also add a challenge of dimensionality to the datasets (Bellman, 2003; Pires and Branco, 2019; Ahmad and Nassif, 2022:) because machine learning modeling performs better with few important features. Feature extraction and selection functions reduce the dimensionality of the datasets by outputting few relevant features with which machine learning algorithms can perform optimally (Bolón-Canedo et al., 2015). What is the relevance of GSF to machine learning modelling, and how would they perform when passed through feature selection functions? It is therefore, necessary to assess the importance of such variables before they can be included in the modeling procedure. Thus, this study investigates how geospatial features score when passed through various feature selection functions. The study experiments with four feature selection techniques (Pearson Correlation, Information Gain, Recursive Feature Elimination, and Least Absolute Shrinkage and Selection Operator, LASSO), and one feature extraction algorithm (Principal Component Analysis, PCA), using water consumption datasets gathered from poor households in Nyanya-Mararaba Town in Nigeria’s Federal Capital Territory.
 
Methodology

Datasets

Both primary and secondary sources of data described in Table 1 were used in this study. Primary sources of data include questionnaires administered to households in the study area, location of households and water points, collected with hand-held GPS. Secondary sources of data include: rainfall data downloaded from www.chrsdata.eng.uci.edu; digital elevation model downloaded from https://earthexplorer.usgs.gov/; Temperature data downloaded from www.weatherspark.com; and land use land cover (LULC) maps downloaded from https://maps.arcgis.com. Thus, five datasets were derived, which are socioeconomic data, weather data, property data, historic data and geospatial data. Researchers have used questionnaires to gather data in order to study water-related problems (Colburn et al., 2021; Fan et al., 2014; Säve-Söderbergh et al., 2017). Table 1 highlights the data and materials used in this study, which, after processing and integration, form water poverty dataset collected in the study area called Nyanya-Mararaba datasets.

Socioeconomic Data, Historical Data and Property Data

Socioeconomic data, historic data and property data were collected with questionnaires and one-on-one interviews administered to households in the study area. Table 1 shows the raw data of the first ten respondents. Each respondent, as a representative of a household, is given an identity number from 1 to 1200.  The first ten respondents are shown in Table 3.

Weather Data

Rainfall and temperature data downloaded for the research gives average monthly rainfall in millimeters and temperature in degree Fahrenheit from January 2022 to December 2022. It is noteworthy that the poor people in the study area practice rainwater harvesting, and the impact of the practice on water consumption and the spatial dimension of shortest distance will need to be accounted for. Rainfall divides climate in the study area into two seasons – wet season and dry season. Wet season begins in April and ends in September, while dry season begins in October and ends in March. It is obvious that poor people have access to more safe water during wet season than during dry season, largely because they harvest rainwater for use. 

Geospatial Data and Geospatial Features

Five geospatial datasets were used in the research. They are location of households, location of water points, return-trip travel time, elevation and land use land cover type. Location of household and location of water points are the northing and easting coordinates of the households and water points. These were captured with handheld GPS. Return trip travel time is the time it takes to travel from a household to the nearest water point and back, including queueing time. It was retrieved from questionnaire responses. 

DEM was downloaded from USGS Earth Explorer. Elevation for each household location and water point was extracted in ArcGIS.

Land use land cover (LULC) data was downloaded from global map of LULC derived from ESA Sentinel-2 imagery at 10m resolution. The algorithm generates LULC predictions for nine classes: water, trees, flooded vegetation, crops, built area, bare ground, snow/ice, clouds and rangeland. 

Four GSF were derived from the geospatial data. They are Travel time, Shortest distance, Height difference, and Land Use Land Cover (LULC) type.

Data Pre-processing

The aim of predictive machine learning model is to predict the value of a target variable or feature using a set of predictor variables or features. Feature engineering improves the performance of machine learning models by selecting the right features for the model and preparing the features in a way that is suitable for the machine learning modelling. For example, to predict volume of water consumed, the target variable would be Volume in Table 4. The predictor variables in the table are many. We need to choose variables, or create new ones, that effectively give the utmost value of the target variable. This is the aim of feature engineering. While there is no formula for effective feature engineering, the following feature engineering processes were applied to our dataset (Aman, 2023; Kohavi, 1997; Rahil, 2018; Sebastian, 2023). 

Data Cleaning

Data cleaning is the process of dealing with errors or inconsistencies in the data. Raw data usually comes dirty, rough and mixed with impurities. Data cleaning involves identifying incorrect data, missing data, duplicated data, and irrelevant data, and deleting, replacing, or modifying data to remove outliers and incorrect values. Data cleaning prepares the data to be readable by machine learning algorithms. NA (Not Available) is a common missing value in the Nyanya-Mararaba datasets. Handling missing data is very important as many machine learning algorithms do not support data with missing values. Variable deletion, replacing missing data with maximum occurred value or mean or median value could be done to fix missing data. We used mean values to fill missing values in the Nyanya-Mararaba dataset.


Feature Creation

Creating features involves creating new variables which will be most useful in the predictive model. Three new features were created: shortest distance, height difference, and LULC type. It is necessary to create these features for the following reason: Shortest distance, height difference and LULC type add geospatial variables to the dataset.

Shortest distance (that is, distance of each household to the nearest functional water point) was derived in ArcMap® from the raw coordinates of the households and water points. As shown in Figure 2, shortest distance was generated using the Near tool among the Analysis tools in the ArcToolbox. 

Elevations of each household location and its nearest water point were extracted from Digital Elevation Model in ArcGIS ® using the Extract by Mask tool among the Extraction tools under the Spatial Analysis toolbox in the ArcToolbox (shown in Figure 3). The difference of the corresponding extracted elevations gives the height difference between each household location and its nearest water point. 

Land use land cover (LULC) type for each household location was generated from LULC map in ArcGIS® using the Extract Values to Point tool among the Extraction tools under the Spatial Analysis toolbox in the ArcToolbox. This is depicted in Figure 3. It is interesting to see that only two LULC types were in the study area – built area and bare ground.

Volume of water consumed in litre per day for dry and wet season, represented by January and July respectively, were derived for each respondent. A dataset consisting of 1200 observations was derived for each season of the year, so that there is a dataset for dry season and another dataset for wet season. 

Feature Selection and Extraction Techniques

Feature selection techniques are algorithms that analyse, judge, and rank various features to determine which features are irrelevant or redundant and should be removed, and which features are most useful for the machine learning modelling and should be prioritized. It is also the process of selecting the correct subset of features to ensure that the most relationship with the target variable is captured. It consists of eliminating features that do not explain the behaviour of the target variable. Feature selection techniques measure the dependency between independent features and dependent feature. The more the dependency the more important will be the feature. In this research, we experimented with five feature selection techniques: Pearson Correlation Coefficient (PCC), Information Gain (IG), Recursive Feature Elimination (RFE), Least Absolute Shrinkage and Selection Operator (LASSO) and Principal Component Analysis (PCA). 

Coding the Feature Selection and Extraction Techniques

The importance of features can be estimated through model building, Stochastic Gradient Descent Regressor (SGDRegressor) model was coded along with the five techniques in Jupyter Notebook. Each of PCC, IG and RFE was coded as a function, which was called with the complete dataset passed as an argument. The outputs, that is, the few features selected, were the input into each of the models created later with four machine learning techniques. PCA and LASSO were coded separately since they have unique complications.


Results

Selected Features

Tables 10, 11, 12, 13 and 14, and Figure 5 show the results of experimenting with the five techniques. It is important to mention the scores that were considered for selecting the features in each technique. Pearson Correlation Coefficient acceptable scores are between -3.0 and -9.0 for negative correlation and +3.0 and +9.0 for positive correlation. Information Gain is zero if and only if two features are independent. The higher the feature score the higher the chance of being selected. Recursive Feature Elimination gives 1 or True for selected features, any other number is False and the feature will not be selected. LASSO identifies unselected features with 0 and any other number for selected features.

Each feature selection technique was applied to the datasets for dry season and wet season. The results show that PC selected five features – Household size (S), Travel time (t), Amount (A), Willingness to pay (W), and Shortest distance (d).  IG selected six features – Household income (I), Household size (S), Travel time (t), Amount (A), Willingness to pay (W), and Shortest distance (d). RFE selected seven features – household income (I), Rainfall (R), Ave Temp (T), Travel time (t), Amount (A), Willingness to pay (W), and Shortest distance (d). LASSO selected nine features – Household income (I), Household size (HS), Rainfall (R), Ave Temp (T), Travel time (t), Amount (A), Willingness to pay (W), Shortest distance (d) and Height diff (h). 

Extracted Features

PCA components and their significance can be explained using two attributes: explained variance and explained variance ratio. Explained variance is the amount of variance explained by each of the extracted components. Explained variance ratio is the percentage of variance explained by each of the extracted components. The experiment in this study resulted in nine principal components (PC1 to PC9) which have explained variance and explained variance ratio shown in Table 14. The cumulative explained variance ratio is 0.69, which means that the seven components explain 69% of the total variability that would be explained if all thirty principal components in the original dataset were included.

The components attribute provides principal axes in feature space, representing the directions of maximum variance in the data. Thus, we can see influence of features on each of the components. From the heat map, effect of the features on each component is summarized in Table 15, in which each feature was scored with the sum of the explained variance ratio of the PCs to which it has effect. Since the acceptable correlation coefficient is between -3.0 and -9.0 for negative correlation and +3.0 and +9.0 for positive correlation, the results show that Rainfall has the most effect on the PCs since it appears in three PCs. It is followed by Household income, Household size, Willingness to pay, Shortest distance and Height diff which have effect on two PCs each. Travel time, Amount spent and LULC have effect on one PC each.

Thus, from the nine PCs extracted, it can be deduced that PCA selected the following seven features: Household income, Rainfall, Ave Temperature, Travel time, Willingness to pay, Shortest distance and Height difference. That is, PCA selected three GSF. 

GSF Selected

Table 16 highlight the number of GSF selected by the feature selection and extraction techniques. PC selected two GSF: Travel time and Shortest distance among the seven. IG selected two GSF: Travel time and Shortest distance. RFE selected two GSF: Travel time and Shortest distance. LASSO selected three GSF: Travel time, Shortest distance and Height difference. PCA also selected three GSF: Travel time, Shortest distance and height difference. Out of four GSF assessed, PC, IG and RFE selected two GSF each, and LASSO selected three GSF and PCA selected three GSF.

The selected features are defined as explanatory variables, while Volume in litre per day (V) is defined as target variable. From the selected explanatory variables listed in Table 17, the functional relationships between Volume in litre per day (V) and selected variables are shown in Table 18. That is, volume of water consumed is a function of the variables in the equations. For example, result from Pearson Correlation shows that Volume is a function of Household size, Travel time, Amount spent, Willingness to pay and Shortest distance.


Conclusions

This study investigated the performance of geospatial features (GSF) when passed through machine learning feature selection and feature extraction algorithms. Experiments were carried out in which a total of twenty-nine features, among which were four geospatial features, were passed through four feature selection algorithms and one feature extraction algorithm. Results show that all the algorithms included geospatial features among their selections. LASSO and PCA performed better than other algorithms since each selected highest number of geospatial features. By selecting geospatial features in their output, all the techniques show that geospatial features are beneficial to machine learning model performance. Addition of geospatial features to machine learning modelling should, therefore, be encouraged for optimal model performance. Other feature selection techniques that are not included in this work, such as Exhaustive Feature Elimination and Random Forest Importance, and other feature extraction techniques such as Singular Value Decomposition, Linear Discriminant Analysis, can be investigated.
